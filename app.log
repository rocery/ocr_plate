[2025/01/15 13:48:49] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/home/sastr/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/home/sastr/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/home/sastr/ocr_plate/venv/lib/python3.12/site-packages/paddleocr/ppocr/utils/en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=False, cls_model_dir='/home/sastr/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2', use_direction_classify=True)
Downloading yolo-v9-t-384-license-plates-end2end.onnx:   0%|          | 0/7771218 [00:00<?, ?it/s]Downloading yolo-v9-t-384-license-plates-end2end.onnx:   7%|â–‹         | 512k/7.41M [00:00<00:02, 2.93MB/s]Downloading yolo-v9-t-384-license-plates-end2end.onnx:  24%|â–ˆâ–ˆâ–       | 1.81M/7.41M [00:00<00:00, 7.61MB/s]Downloading yolo-v9-t-384-license-plates-end2end.onnx:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 2.69M/7.41M [00:00<00:00, 7.07MB/s]Downloading yolo-v9-t-384-license-plates-end2end.onnx:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 3.44M/7.41M [00:00<00:00, 6.37MB/s]Downloading yolo-v9-t-384-license-plates-end2end.onnx:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 4.12M/7.41M [00:00<00:00, 5.85MB/s]Downloading yolo-v9-t-384-license-plates-end2end.onnx:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 4.75M/7.41M [00:00<00:00, 5.64MB/s]Downloading yolo-v9-t-384-license-plates-end2end.onnx:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5.31M/7.41M [00:00<00:00, 5.64MB/s]Downloading yolo-v9-t-384-license-plates-end2end.onnx:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 5.88M/7.41M [00:01<00:00, 5.69MB/s]Downloading yolo-v9-t-384-license-plates-end2end.onnx:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 6.50M/7.41M [00:01<00:00, 5.79MB/s]Downloading yolo-v9-t-384-license-plates-end2end.onnx:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 7.12M/7.41M [00:01<00:00, 5.93MB/s]Downloading yolo-v9-t-384-license-plates-end2end.onnx: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.41M/7.41M [00:01<00:00, 5.94MB/s]
[1;31m2025-01-15 13:48:58.717847407 [E:onnxruntime:Default, provider_bridge_ort.cc:1848 TryGetProviderInfo_TensorRT] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1539 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_tensorrt.so with error: libnvinfer.so.10: cannot open shared object file: No such file or directory
[m
[1;31m2025-01-15 13:48:58.754437283 [E:onnxruntime:Default, provider_bridge_ort.cc:1862 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1539 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn_adv.so.9: cannot open shared object file: No such file or directory
[m
[0;93m2025-01-15 13:48:58.754504549 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:993 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Require cuDNN 9.* and CUDA 12.*. Please install all dependencies as mentioned in the GPU requirements page (https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements), make sure they're in the PATH, and that your GPU is supported.[m
[2025-01-15 13:48:58,891] [    INFO] inference.py:68 - Using ONNX Runtime with ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'] provider(s)
[2025-01-15 13:48:58,892] [    INFO] license_plate.py:54 - Initialized LicensePlateDetector with model /home/sastr/.cache/open-image-models/yolo-v9-t-384-license-plate-end2end/yolo-v9-t-384-license-plates-end2end.onnx
*************** EP Error ***************
EP Error /onnxruntime_src/onnxruntime/python/onnxruntime_pybind_state.cc:507 void onnxruntime::python::RegisterTensorRTPluginsAsCustomOps(PySessionOptions&, const onnxruntime::ProviderOptions&) Please install TensorRT libraries as mentioned in the GPU requirements page, make sure they're in the PATH or LD_LIBRARY_PATH, and that your GPU is supported.
 when using ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
Falling back to ['CUDAExecutionProvider', 'CPUExecutionProvider'] and retrying.
****************************************
Downloading global_mobile_vit_v2_ocr.onnx:   0%|          | 0/4991602 [00:00<?, ?it/s]Downloading global_mobile_vit_v2_ocr.onnx:   7%|â–‹         | 320k/4.76M [00:00<00:02, 2.27MB/s]Downloading global_mobile_vit_v2_ocr.onnx:  28%|â–ˆâ–ˆâ–Š       | 1.31M/4.76M [00:00<00:00, 6.30MB/s]Downloading global_mobile_vit_v2_ocr.onnx:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2.00M/4.76M [00:00<00:00, 5.91MB/s]Downloading global_mobile_vit_v2_ocr.onnx:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2.62M/4.76M [00:00<00:00, 5.70MB/s]Downloading global_mobile_vit_v2_ocr.onnx:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3.25M/4.76M [00:00<00:00, 5.59MB/s]Downloading global_mobile_vit_v2_ocr.onnx:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3.81M/4.76M [00:00<00:00, 5.64MB/s]Downloading global_mobile_vit_v2_ocr.onnx:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4.38M/4.76M [00:00<00:00, 5.62MB/s]Downloading global_mobile_vit_v2_ocr.onnx: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.76M/4.76M [00:00<00:00, 5.56MB/s]
Downloading global_mobile_vit_v2_ocr_config.yaml:   0%|          | 0/507 [00:00<?, ?it/s]Downloading global_mobile_vit_v2_ocr_config.yaml: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 507/507 [00:00<00:00, 2.11MB/s]
[1;31m2025-01-15 13:49:03.006859449 [E:onnxruntime:Default, provider_bridge_ort.cc:1848 TryGetProviderInfo_TensorRT] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1539 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_tensorrt.so with error: libnvinfer.so.10: cannot open shared object file: No such file or directory
[m
[1;31m2025-01-15 13:49:03.020037416 [E:onnxruntime:Default, provider_bridge_ort.cc:1862 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1539 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn_adv.so.9: cannot open shared object file: No such file or directory
[m
[0;93m2025-01-15 13:49:03.020080022 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:993 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Require cuDNN 9.* and CUDA 12.*. Please install all dependencies as mentioned in the GPU requirements page (https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements), make sure they're in the PATH, and that your GPU is supported.[m
*************** EP Error ***************
EP Error /onnxruntime_src/onnxruntime/python/onnxruntime_pybind_state.cc:507 void onnxruntime::python::RegisterTensorRTPluginsAsCustomOps(PySessionOptions&, const onnxruntime::ProviderOptions&) Please install TensorRT libraries as mentioned in the GPU requirements page, make sure they're in the PATH or LD_LIBRARY_PATH, and that your GPU is supported.
 when using ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
Falling back to ['CUDAExecutionProvider', 'CPUExecutionProvider'] and retrying.
****************************************
 * Serving Flask app 'app'
 * Debug mode: off
[2025-01-15 13:49:03,158] [    INFO] _internal.py:97 - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.15.223:5000
[2025-01-15 13:49:03,158] [    INFO] _internal.py:97 - [33mPress CTRL+C to quit[0m
